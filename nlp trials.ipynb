{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbP/GgUHF5Zly3YdrKXo1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trapti-singh/portfolio1/blob/main/nlp%20trials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKjwjZTgxOxb",
        "outputId": "06a7fde0-fd57-417b-93fd-e4af8538aef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'you', ',', 'you', 'love', 'me', ',', 'we', 'are', 'one', 'big', 'family']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"I love you , you love me, we are one big family\"\n",
        "print (word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"I love you , you love me, we are one big family\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZcKSuMzxnnh",
        "outputId": "7f0f0d13-bb73-4178-9633-466e10716530"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP'), (',', ','), ('you', 'PRP'), ('love', 'VBP'), ('me', 'PRP'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('one', 'CD'), ('big', 'JJ'), ('family', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bed116de",
        "outputId": "08929bea-5b2a-4ded-c5eb-c571198b629c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load a spaCy model. 'en_core_web_sm' is a good starting point.\n",
        "# If you haven't downloaded it, you might need to run:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading en_core_web_sm model...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Your text\n",
        "text = \"hi this is me your mom, r u joing us for dinner at hyatt mumbai, working at apple, as a ceo\"\n",
        "\n",
        "# Process the text with the spaCy model\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq08u8pi0PHg",
        "outputId": "da0f067b-6a34-4e7b-f299-10e0cd518b35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mumbai GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXtjAo4d0zDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccb48557",
        "outputId": "7980e596-222b-418d-82e5-31a47d74bf7d"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load a spaCy model. 'en_core_web_sm' is a good starting point.\n",
        "# If you haven't downloaded it, you might need to run:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading en_core_web_sm model...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Your text\n",
        "text = \"hi this is me your mom, r u joing us for dinner at hyatt mumbai, working at apple, as a ceo\"\n",
        "\n",
        "# Process the text with the spaCy model\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the identified entities and print their text and label\n",
        "print(\"Entities found:\")\n",
        "for ent in doc.ents:\n",
        "    # You can filter by entity type if needed\n",
        "    # For example, to get only GPE (countries, cities, states):\n",
        "    if ent.label_ == \"GPE\":\n",
        "        print(ent.text, ent.label_)\n",
        "    # To get only ORG (organizations, potentially companies):\n",
        "    if ent.label_ == \"ORG\":\n",
        "        print(ent.text, ent.label_)\n",
        "    # To get only PERSON (people, potentially designations):\n",
        "    if ent.label_ == \"PERSON\":\n",
        "        print(ent.text, ent.label_)\n",
        "    # If you want to print all entities regardless of type, uncomment the line below:\n",
        "    # print(ent.text, ent.label_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found:\n",
            "mumbai GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment=pipeline(\"sentiment-analysis\")\n",
        "print (sentiment(\"it was wow movie ever watched\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-QBhqoh84jI",
        "outputId": "ced6040c-feb9-4cef-a60f-8b12a901e6ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.999757707118988}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translotor= pipeline(\"translation_en_to_fr\")\n",
        "print (translotor(\"I love you , you love me, we are one big family\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOuI25Ag9dpQ",
        "outputId": "e99a7c75-18c1-424a-e4d8-8c2331f0c100"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'Je vous aime , vous me aimez, nous sommes une grande famille'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load translation pipeline for English to Hindi\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "text = \"I love you, you love me, we are one big family\"\n",
        "result = translator(text)\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Hindi: {result[0]['translation_text']}\")\n",
        "print(f\"Hindi: {result.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "T69Bjxdi_WO5",
        "outputId": "a0210edc-4fcb-48d9-c3f6-c30f12548d8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I love you, you love me, we are one big family\n",
            "Hindi: ‡§Æ‡•à‡§Ç ‡§§‡•Å‡§Æ‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å, ‡§§‡•Å‡§Æ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡•á ‡§π‡•ã, ‡§π‡§Æ ‡§è‡§ï ‡§¨‡§°‡§º‡§æ ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§π‡•à‡§Ç\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2480701429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Original: {text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hindi: {result[0]['translation_text']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hindi: {result.text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load translation pipeline for English to Urdu\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ur\")\n",
        "\n",
        "text = \"I love you, you love me, we are one big family\"\n",
        "result = translator(text)\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Urdu: {result[0]['translation_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRVToxUd_4JJ",
        "outputId": "07e6f809-08f6-46bb-99d8-90b84382ef16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I love you, you love me, we are one big family\n",
            "Urdu: ŸÖ€å⁄∫ ÿ™ŸÖ ÿ≥€í ŸÖÿ≠ÿ®ÿ™ ⁄©ÿ±ÿ™ÿß €ÅŸà⁄∫ÿå ÿ™ŸÖ ŸÖÿ¨⁄æ ÿ≥€í ŸÖÿ≠ÿ®ÿ™ ⁄©ÿ±ÿ™€í €ÅŸàÿå €ÅŸÖ ÿß€å⁄© ÿ®⁄ëÿß ÿÆÿßŸÜÿØÿßŸÜ €Å€å⁄∫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline (\"text-generation\", model= \"gpt2\")\n",
        "print(generator(\"once upon a time in mumbai\", truncation=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKzS1JN1AT5A",
        "outputId": "7cee7bd6-0040-40cc-d91e-ba2258ac9acd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'once upon a time in mumbai where the entire city was once a city of Hindus, the new Delhi government also wants the city to become a secular city.\\n\\nThe state government has also made it a point to use public funds for education, infrastructure and public works projects.\\n\\n\"The state government has made it a point to use public funds for education, infrastructure and public works projects, which is in line with the objectives of the Hindu and Muslim communities during the last two years,\" a senior state government official said.\\n\\nThe state government also wants to make it a religious city where all Hindus will be able to participate in religious rituals, which means they will also be able to go to worship at the same time.\\n\\nThe state government has also said that it wants to make it a Hindu city, while also providing the state with resources to do various activities such as the public works project.\\n\\n\"All the government agencies will have to ensure that the activities, such as public works projects, are conducted without discrimination, which will enable the people to be able to participate in the activities,\" said the official.\\n\\nThe state government is also looking at making the city a Hindu city as well.\\n\\nThe state government wants to make it a Hindu city and it is also looking at'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pyttsx3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tk--pkrBJ6y",
        "outputId": "886fc1f9-ff68-449e-edff-d1684dd3bc82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pyttsx3 in /usr/local/lib/python3.11/dist-packages (2.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "chatbot = pipeline (\"text-generation\", model=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuNmaT2qCWpC",
        "outputId": "2a230eeb-e0c1-4e57-9f42-ffb3f4294812"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyttsx3\n",
        "engine = pyttsx3.init()\n",
        "\n",
        "def speak (text):\n",
        "  engine.say(text)\n",
        "  engine.runAndWait()\n",
        "\n",
        "while True:\n",
        "  question = input(\"You:  \")\n",
        "  if question.lower() in ['exit','quit','bye']:\n",
        "    print(\"Robot:Bye! see you soon\")\n",
        "    break\n",
        "  response = chatbot(question, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
        "  print(\"\\nRobot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "6Q95eSi0Git0",
        "outputId": "bf096d8d-71aa-4677-a6db-4f5fb72ab2e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "This means you probably do not have eSpeak or eSpeak-ng installed!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(driverName, debug)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0meng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_activeEngines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdriverName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/weakref.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_commit_removals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: None",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-3495278277.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyttsx3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyttsx3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspeak\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/__init__.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(driverName, debug)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0meng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_activeEngines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdriverName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0meng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriverName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0m_activeEngines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdriverName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, driverName, debug)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriverName\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdefault_engine_by_sys_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDriverProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine, driverName, debug)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdriverName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# import driver module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pyttsx3.drivers.{driverName}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m# build driver instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/espeak.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyttsx3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVoice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_espeak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyttsx3/drivers/_espeak.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This means you probably do not have eSpeak or eSpeak-ng installed!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This means you probably do not have eSpeak or eSpeak-ng installed!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "set_seed(42)\n",
        "chatbot = pipeline(\"conversational\", model=\"microsoft/DialoGPT-medium\")\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "KrcrU-MkR1Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the conversational pipeline\n",
        "chatbot = pipeline(\"conversational\", model=\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"You: \")\n",
        "    if question.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"Robot: Bye! See you soon\")\n",
        "        break\n",
        "    \n",
        "    # Generate response\n",
        "    response = chatbot(question)\n",
        "    \n",
        "    # Extract the generated text properly\n",
        "    if isinstance(response, dict) and 'generated_text' in response:\n",
        "        bot_response = response['generated_text']\n",
        "    elif hasattr(response, 'generated_responses'):\n",
        "        bot_response = response.generated_responses[-1]\n",
        "    else:\n",
        "        # For DialoGPT, the response structure might be different\n",
        "        bot_response = str(response)\n",
        "    \n",
        "    print(\"Robot:\", bot_response)"
      ],
      "metadata": {
        "id": "JgTh65klTm1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# Load a text-generation pipeline using GPT-2\n",
        "chatbot = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "\n",
        "while True:\n",
        "    question = input(\"You: \")\n",
        "\n",
        "    if question.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"Robot: Bye! See you soon ü§ñ\")\n",
        "        break\n",
        "\n",
        "    response = chatbot(question, truncation=True, do_sample=True)[0]['generated_text']\n",
        "    print(\"\\nRobot:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLIfbfNgWIw-",
        "outputId": "9e774f5d-8383-4e17-c740-7986d522a9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: what is ai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Robot: what is ai???\n",
            "\n",
            "(Note that the original question was not true, but I have found out that a i usually doesn't answer for sure but I try to avoid them with the exception of when this is a specific question. The first thing to know is that you can always answer by using the \"I'm sorry\" button. This is only useful if the question has a \"I've had your e-mail\" header, and if you're using both of those you need to check the box. I don't use the \"I'm sorry\" button for questions that involve an e-mail address, but you can check with the box to get a list of all the e-mail addresses that you should go by.)\n",
            "\n",
            "If you don't know how to answer this question, then go here.\n",
            "\n",
            "I'll save you some time.\n",
            "\n",
            "If you're ready, you can do this:\n",
            "\n",
            "http://www.reddit.com/r/AskReddit/comments/5b1g9j/what_were_the_most_easy_steps/\n",
            "\n",
            "If you're not ready, you can do this:\n",
            "\n",
            "http://www.reddit.com/r/AskReddit/comments/5b4f2z/\n",
            "You: what is galaxy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Robot: what is galaxy?\n",
            "\n",
            "The answer is no.\n",
            "\n",
            "In the current galaxy, the universe is populated by a group of supermassive black holes (a.k.a. the Big Bang). The Big Bang has created a black hole that is the size of our Milky Way. These supermassive black holes are called \"dark\" stars. In the outer regions of the galaxy, however, these dark stars are called star clusters. These star clusters are the remnants of the original Big Bang. The dark matter is the leftover of the original black hole.\n",
            "\n",
            "The Big Bang and dark matter\n",
            "\n",
            "What is black matter and what is dark matter?\n",
            "\n",
            "In our galaxy, there are five major types of matter: matter that is made up of a mass of matter, matter that is created by a process called expansion, matter that is created when a star expands, matter that is created when the universe loses matter, and matter that is created when the universe is consumed.\n",
            "\n",
            "These are the categories of matter in our galaxy:\n",
            "\n",
            "Mass: the mass of a particle in the universe.\n",
            "\n",
            "Gravitational Force: the gravitational force that pulls matter together, the force that causes the universe to expand.\n",
            "\n",
            "Cosmic Background: the background in which the universe is\n",
            "You: what is ml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Robot: what is mlp, what is mmp, gf, mmp, gf, mmp, mmp, mmp, gf, mmp, gf, mmp), line (0.2, 0.6, 0.8, 0.8)\n",
            "\n",
            "The last line is the last line of the corresponding line.\n",
            "\n",
            "The line above is what is called a 'line length', while the line below is the length of the next line.\n",
            "\n",
            "If you are reading this at all, you are probably wondering what the 'line length' is? Well, the 'line length' is the length of the line that you are reading.\n",
            "\n",
            "In other words, in order for a line to be read at the same length, you have to have the same length of line as the next line. This is known as the 'line length ratio'.\n",
            "\n",
            "To avoid this, you must give the length of the next line as a number (and this number is the length of the line you are reading), and divide this by the length of the previous line.\n",
            "\n",
            "For example, the first line of the next line is the length of the line, while the second line is the length of the previous line.\n",
            "\n",
            "After you\n",
            "You: what is machine learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Robot: what is machine learning and how is it used?\"\n",
            "\n",
            "The paper was co-authored by Prof. Gao Guo, from the University of Hong Kong, who is also a professor in the Department of Computer Science at the University of Hong Kong.\n",
            "\n",
            "\"Machine learning is a powerful tool for better understanding intelligence, but it also provides valuable insights into how we can improve our skills, to better understand our society,\" Prof. Guo said.\n",
            "\n",
            "This research was supported by NASA's Office of Naval Research, the National Science Foundation, the National Science Foundation, and the National Science Foundation's National Science Foundation's National Science Collaborative.\n",
            "\n",
            "About the University of Hong Kong\n",
            "\n",
            "The University of Hong Kong is a worldwide leader in the field of computational and applied science, as well as teaching, research, and education. The school is also home to the Hong Kong Computing Research Centre (CHRC), the Hong Kong Computing Research Centre (HRC), and two other centers: the University of Hong Kong's Research Centre for Computational Machine Learning and the University of Hong Kong's Department of Computer Science and Engineering. The main research and teaching centre is the Department of Computer Science and Engineering, which is funded by the National Science Foundation.\n",
            "\n",
            "About the University of Hong Kong\n",
            "\n",
            "\n",
            "You: what is hollywood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Robot: what is hollywood's response to that?\n",
            "\n",
            "This is a very complicated question, because what is hollywood's response to that? First off, Hollywood is not the only company that has been criticized. There are other companies that also have a very vocal and vocal, but not as vocal as Hollywood. They are also not as vocal as Hollywood. And they have very specific policies about where they're putting their money and who they're putting their money. So they are certainly not the only company that's been criticized. So the question is, how will they respond in the media?\n",
            "\n",
            "And I think the answer is, that will be a little more difficult.\n",
            "\n",
            "There was a period, in 1995, when I was a student at Harvard, when I was trying to understand how political correctness could be implemented in a way that would not be politically correct. And I think that was the first time I understood how the media works. The media was not so much about the political correctness of what was going on in the world. It was more about the media being so politically correct. So I think that is a good way to understand how journalists are being treated in the media.\n",
            "\n",
            "How do you think the new political correctness is going to impact the way we talk about politics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw8Y5ydUWYnU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}